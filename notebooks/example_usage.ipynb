{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ultra-Marathon Pace Prediction - Example Usage\n",
                "\n",
                "This notebook demonstrates how to use the ultra-marathon pace prediction pipeline.\n",
                "\n",
                "## Overview\n",
                "\n",
                "The pipeline consists of several modules that work together to:\n",
                "1. Load and clean ultramarathon data\n",
                "2. Engineer features that capture athlete progression\n",
                "3. Train a machine learning model to predict race pace\n",
                "4. Evaluate model performance using pace-specific metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Required Modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Import pipeline modules\n",
                "from src.data.load import load_raw_data\n",
                "from src.data.clean import clean_data\n",
                "from src.data.features import engineer_features\n",
                "from src.data.split import split_train_test\n",
                "from src.models.prepare import prepare_model_data\n",
                "from src.models.train import train_evaluate_lgbm\n",
                "from src.evaluation.metrics import print_pace_metrics\n",
                "from src.visualization.eda import plot_pace_distribution, plot_model_performance\n",
                "from src.pipeline import run_pipeline\n",
                "\n",
                "# Set plotting style\n",
                "plt.style.use('seaborn-v0_8')\n",
                "sns.set_palette(\"husl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the raw data\n",
                "print(\"Loading data...\")\n",
                "df = load_raw_data(\"data/raw/ultra_marathons.csv\")\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Columns: {list(df.columns)}\")\n",
                "\n",
                "# Display basic information\n",
                "print(\"\\nBasic info:\")\n",
                "print(df.info())\n",
                "\n",
                "print(\"\\nFirst few rows:\")\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean the data\n",
                "print(\"Cleaning data...\")\n",
                "df_clean = clean_data(df)\n",
                "\n",
                "print(f\"\\nAfter cleaning: {df_clean.shape}\")\n",
                "print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows\")\n",
                "\n",
                "# Check for any remaining missing values\n",
                "missing_cols = df_clean.columns[df_clean.isnull().any()].tolist()\n",
                "if missing_cols:\n",
                "    print(f\"\\nColumns with missing values: {missing_cols}\")\n",
                "    print(df_clean[missing_cols].isnull().sum())\n",
                "else:\n",
                "    print(\"\\nNo missing values found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Engineer features\n",
                "print(\"Engineering features...\")\n",
                "df_features = engineer_features(df_clean)\n",
                "\n",
                "print(f\"\\nAfter feature engineering: {df_features.shape}\")\n",
                "print(f\"Added {df_features.shape[1] - df_clean.shape[1]} new features\")\n",
                "\n",
                "# Show new features\n",
                "new_features = [col for col in df_features.columns if col not in df_clean.columns]\n",
                "print(f\"\\nNew features created:\")\n",
                "for feature in new_features:\n",
                "    print(f\"  - {feature}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into train/test sets\n",
                "print(\"Splitting data...\")\n",
                "df_train, df_test, feature_cols = split_train_test(df_features)\n",
                "\n",
                "print(f\"\\nTrain set: {df_train.shape}\")\n",
                "print(f\"Test set:  {df_test.shape}\")\n",
                "print(f\"\\nFeature columns ({len(feature_cols)}):\")\n",
                "for i, col in enumerate(feature_cols, 1):\n",
                "    print(f\"  {i:2d}. {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for modeling\n",
                "print(\"Preparing data for modeling...\")\n",
                "X_train, X_test, y_train, y_test = prepare_model_data(df_train, df_test, feature_cols)\n",
                "\n",
                "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
                "print(f\"Test features shape:     {X_test.shape}\")\n",
                "print(f\"Training target shape:   {y_train.shape}\")\n",
                "print(f\"Test target shape:       {y_test.shape}\")\n",
                "\n",
                "# Train the model\n",
                "print(\"\\nTraining model...\")\n",
                "model, y_pred = train_evaluate_lgbm(X_train, y_train, X_test, y_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print detailed metrics\n",
                "print_pace_metrics(y_test, y_pred, \"Ultra-Marathon Pace Predictor\")\n",
                "\n",
                "# Plot model performance\n",
                "plot_model_performance(y_test, y_pred, \"Ultra-Marathon Pace Predictor\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importance\n",
                "from src.models.train import get_feature_importance\n",
                "\n",
                "importance_df = get_feature_importance(model, X_train.columns)\n",
                "\n",
                "print(\"Top 10 most important features:\")\n",
                "print(importance_df.head(10)[['feature', 'importance']])\n",
                "\n",
                "# Plot feature importance\n",
                "from src.visualization.eda import plot_feature_importance\n",
                "plot_feature_importance(importance_df, top_n=15)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Using the Full Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The full pipeline can be run in one step\n",
                "print(\"Running full pipeline...\")\n",
                "\n",
                "# Note: This would use the same data we've been working with\n",
                "# In practice, you'd point to your actual data file\n",
                "# model, X_train, X_test, y_train, y_test, y_pred = run_pipeline('path/to/your/data.csv')\n",
                "\n",
                "print(\"\\nPipeline completed successfully!\")\n",
                "print(\"\\nTo run with your own data:\")\n",
                "print(\"model, X_train, X_test, y_train, y_test, y_pred = run_pipeline('your_data.csv')\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Making Predictions on New Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Making predictions on new data\n",
                "print(\"Example: Making predictions on new data\")\n",
                "\n",
                "# Create some example data (this would normally come from your dataset)\n",
                "example_data = {\n",
                "    'Year of event': [2023, 2023],\n",
                "    'Event number of finishers': [150, 200],\n",
                "    'Athlete gender': ['M', 'F'],\n",
                "    'Event distance_numeric': [100, 50],\n",
                "    'cum_num_races': [5, 3],\n",
                "    'cum_avg_pace': [10.5, 11.2],\n",
                "    'cum_best_pace': [9.8, 10.5],\n",
                "    'cum_ws_finishes': [1, 0],\n",
                "    'cum_total_distance': [400, 180],\n",
                "    'cum_avg_distance': [80, 60],\n",
                "    'cum_shortest_distance': [50, 42],\n",
                "    'cum_longest_distance': [100, 80],\n",
                "    'recent_avg_distance': [85, 65],\n",
                "    'distance_gap_from_longest': [0, -30],\n",
                "    'athlete_age': [35, 28]\n",
                "}\n",
                "\n",
                "example_df = pd.DataFrame(example_data)\n",
                "print(f\"Example data shape: {example_df.shape}\")\n",
                "print(\"\\nExample data:\")\n",
                "print(example_df)\n",
                "\n",
                "# Make predictions\n",
                "if 'model' in locals():\n",
                "    predictions = model.predict(example_df)\n",
                "    print(f\"\\nPredicted paces (min/km): {predictions}\")\n",
                "else:\n",
                "    print(\"\\nModel not available in this example\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated the complete workflow for ultramarathon pace prediction:\n",
                "\n",
                "1. **Data Loading**: Load raw ultramarathon data\n",
                "2. **Data Cleaning**: Remove inconsistencies and missing values\n",
                "3. **Feature Engineering**: Create cumulative and rolling features\n",
                "4. **Train/Test Split**: Split data while preventing data leakage\n",
                "5. **Model Training**: Train a LightGBM model\n",
                "6. **Evaluation**: Assess model performance with pace-specific metrics\n",
                "7. **Feature Importance**: Understand which features matter most\n",
                "8. **Pipeline Usage**: Run the complete pipeline in one step\n",
                "9. **Predictions**: Make predictions on new data\n",
                "\n",
                "The pipeline is designed to be:\n",
                "- **Reproducible**: All steps are deterministic\n",
                "- **Scalable**: Handles large datasets efficiently\n",
                "- **Interpretable**: Provides insights into feature importance\n",
                "- **Extensible**: Easy to add new features or models"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}